{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "obs = env.reset()\n",
    "obs = obs[:,np.newaxis].T\n",
    "    \n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "n_hidden_0 = 20\n",
    "n_hidden_1 = 20\n",
    "\n",
    "# <define network graph using raw tf or any deep learning library>\n",
    "model = Sequential( [\n",
    "            Dense( n_hidden_0, activation='relu', input_shape=state_dim ),\n",
    "            Dense( n_hidden_1, activation='relu' ),    \n",
    "            Dense( n_actions,  activation='linear' ),\n",
    "])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_entropy( model, _states ):\n",
    "    \n",
    "    logits = model(_states)    \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    log_probs = tf.nn.log_softmax(logits)\n",
    "    ent = tf.reduce_sum( -probs * log_probs, axis=1 )\n",
    "    \n",
    "    return ent\n",
    "\n",
    "def calc_loss( model, _states, _actions, _rewards, _gamma, _entropy_weight ):\n",
    "    \"\"\"Calculate the loss function to be minimized\"\"\"\n",
    "    \n",
    "    entropy = tf.reduce_mean( calc_entropy( model, _states ) )\n",
    "    \n",
    "    logprobs = calc_logprobs_for_actions( model, _states=_states, _actions=_actions )\n",
    "    \n",
    "    cum_rewards = get_cumulative_rewards(_rewards=_rewards, _gamma=_gamma )    \n",
    "    \n",
    "    J = tf.reduce_mean( logprobs * cum_rewards )           \n",
    "    \n",
    "    loss = -J - _entropy_weight * entropy         \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def calc_gain( model, _states, _actions, _rewards, _gamma, _entropy_weight ):\n",
    "    \"\"\"Calculate the loss function to be minimized\"\"\"\n",
    "    \n",
    "    entropy = tf.reduce_mean( calc_entropy( model, _states ) )\n",
    "    \n",
    "    probs = tf.math.exp( calc_logprobs_for_actions( model, _states=_states, _actions=_actions ) )\n",
    "    cum_rewards = get_cumulative_rewards(_rewards=_rewards, _gamma=_gamma )    \n",
    "    objective = tf.reduce_mean( probs * cum_rewards )    \n",
    "    \n",
    "    gain = objective + _entropy_weight * entropy\n",
    "    return gain\n",
    "\n",
    "def train_step( model, optimizer, states, actions, rewards, gamma, entropy_weight ):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.weights)\n",
    "        current_loss = calc_loss( model, _states=states, _actions=actions, _rewards=rewards, \n",
    "                         _gamma=gamma, _entropy_weight=entropy_weight )\n",
    "    \n",
    "    grads = tape.gradient( current_loss, model.weights )\n",
    "    optimizer.apply_gradients( zip( grads, model.weights ) )\n",
    "\n",
    "    return current_loss\n",
    "\n",
    "def choose_action_from_policy( model, states ):\n",
    "    \n",
    "    # Choose a random action\n",
    "    logits = model(states)\n",
    "    \n",
    "    action = tf.random.categorical( logits, num_samples=1 )\n",
    "    return action\n",
    "    \n",
    "    \n",
    "def calc_logprobs_for_actions( model, _states, _actions ):    \n",
    "    \n",
    "    # Get the log probabilties of each action\n",
    "    logits = model(_states)    \n",
    "    logprob_mtx = tf.nn.log_softmax(logits)\n",
    "    \n",
    "    N = tf.shape(logprob_mtx)[0]\n",
    "    indices = tf.stack( [ tf.range(N), tf.reshape( _actions, (N,) ) ], axis=-1)\n",
    "    log_probs_for_actions = tf.gather_nd( logprob_mtx, indices)    \n",
    "    \n",
    "    return log_probs_for_actions\n",
    "\n",
    "\n",
    "def get_cumulative_rewards(_rewards, _gamma ):\n",
    "    \"\"\"Get the cumulative rewards for the J(\\theta) function.\"\"\"\n",
    "\n",
    "    # Calculate the discount factor. Assumes the first rewards are earliest    \n",
    "    cum_rewards = [np.nan] * len(_rewards)\n",
    "\n",
    "    cum_rewards[-1] = _rewards[-1]\n",
    "    for t in range(1, _rewards.shape[0] ):\n",
    "        cum_rewards[-(t+1)] = _rewards[-(t+1)] + _gamma * cum_rewards[-t] \n",
    "\n",
    "    output_cum_rewards = tf.convert_to_tensor( cum_rewards, tf.float32 )\n",
    "    return output_cum_rewards\n",
    "\n",
    "\n",
    "def generate_session( model, env, t_max, gamma, entropy_weight ):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = obs[:,np.newaxis].T\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka \\pi(a|s)        \n",
    "        action = choose_action_from_policy( model, states=obs )\n",
    "        \n",
    "        # Move to next state\n",
    "        new_obs, r, done, info = env.step( action.numpy()[0,0] )\n",
    "        new_obs = new_obs[:,np.newaxis].T\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Form Tensors out of the collected outputs\n",
    "    state_mtx = tf.stack( np.vstack( states ) )\n",
    "    action_mtx = tf.dtypes.cast( tf.stack( np.vstack( actions ) ), tf.int32 )\n",
    "    reward_mtx = tf.stack( np.vstack( rewards ) )\n",
    "    \n",
    "    curr_loss = train_step( model, optimizer=optimizer, states=state_mtx, actions=action_mtx,\n",
    "                   rewards=reward_mtx, gamma=gamma, entropy_weight=entropy_weight )\n",
    "\n",
    "    # return session rewards to print them later\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 1000\n",
    "#gamma = tf.constant( .99, dtype=tf.float64 )\n",
    "#entropy_weight = tf.constant(0.10, dtype=tf.float64)\n",
    "gamma = 0.99\n",
    "entropy_weight = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:20.050\n",
      "mean reward:23.550\n",
      "mean reward:22.540\n",
      "mean reward:22.180\n",
      "mean reward:21.670\n",
      "mean reward:22.540\n",
      "mean reward:23.790\n",
      "mean reward:20.540\n",
      "mean reward:23.350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d51c356baa8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                       entropy_weight=entropy_weight)\n\u001b[0m\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4ea10b88d440>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(model, env, t_max, gamma, entropy_weight)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# action probabilities array aka \\pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_action_from_policy\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Move to next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4ea10b88d440>\u001b[0m in \u001b[0;36mchoose_action_from_policy\u001b[0;34m(model, states)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Choose a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    751\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         data_format)\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    rewards = []\n",
    "    for _ in range(100):\n",
    "        rewards.append( \n",
    "            generate_session( model, \n",
    "                      env, \n",
    "                      t_max=t_max, \n",
    "                      gamma=gamma, \n",
    "                      entropy_weight=entropy_weight)\n",
    "        )\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\") # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t_max = 1000\n",
    "_gamma = 0.99\n",
    "_entropy_weight = 0.0\n",
    "\n",
    "gain = []\n",
    "for _ in range(100):\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = obs[:,np.newaxis].T\n",
    "\n",
    "    for t in range(_t_max):\n",
    "\n",
    "        # action probabilities array aka \\pi(a|s)\n",
    "        action = choose_action_from_policy( model, obs )\n",
    "\n",
    "        # Move to next state\n",
    "        new_obs, r, done, info = env.step( action )\n",
    "        new_obs = new_obs[:,np.newaxis].T\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Form Tensors out of the collected outputs\n",
    "    state_mtx = tf.stack( np.vstack( states ) )\n",
    "    action_mtx = tf.dtypes.cast( tf.stack( np.vstack( actions ) ), 'int32' )\n",
    "    reward_mtx = tf.stack( np.vstack( rewards ) )\n",
    "\n",
    "    optimizer.learning_rate = 0.01\n",
    "    with tf.GradientTape() as t:\n",
    "\n",
    "        loss_fun = lambda : calc_loss( model, _states=state_mtx, _actions=action_mtx, \n",
    "                              _rewards=reward_mtx, _gamma=gamma, _entropy_weight=entropy_weight )\n",
    "        gain_fun = lambda : calc_gain( model, _states=state_mtx, _actions=action_mtx, \n",
    "                              _rewards=reward_mtx, _gamma=gamma, _entropy_weight=entropy_weight )        \n",
    "        orig_loss = loss_fun()\n",
    "\n",
    "        tmp = gain_fun()\n",
    "        x0 = tmp.numpy()\n",
    "        #print( 'Loss Before: {}'.format(orig_loss) )\n",
    "\n",
    "        theta = model.trainable_variables\n",
    "\n",
    "        cum_rewards = get_cumulative_rewards(_rewards=reward_mtx, _gamma=gamma )   \n",
    "        logprobs = calc_logprobs_for_actions( model, _states=state_mtx, _actions=action_mtx )\n",
    "        A = -tf.reduce_mean(logprobs * cum_rewards)\n",
    "        #grads = t.gradient( orig_loss, theta )\n",
    "\n",
    "        #optimizer.apply_gradients( zip( grads, theta ) )\n",
    "        optimizer.minimize( loss_fun, var_list=model.weights )\n",
    "\n",
    "        x1 = gain_fun().numpy()\n",
    "\n",
    "        #print( 'Loss After: {}'.format( post_loss ) )\n",
    "        #print( 'Loss Chg: {}'.format( ( x1 - x0 ) ) )\n",
    "        gain.append(x1-x0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductions = []\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_fun = lambda : calc_loss( model, _states=state_mtx, _actions=action_mtx, \n",
    "                                  _rewards=reward_mtx, _gamma=gamma, _entropy_weight=entropy_weight )\n",
    "        orig_loss = loss_fun()\n",
    "        x0 = orig_loss.numpy()    \n",
    "        #grads = t.gradient( orig_loss, model.weights )\n",
    "        #optimizer.apply_gradients( zip( grads, model.weights ) )\n",
    "        optimizer.minimize( loss_fun, model.weights )\n",
    "        post_loss = loss_fun()\n",
    "        x1 = post_loss.numpy()\n",
    "        reductions.append(x1 - x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange( -1., 1.01, .1 )\n",
    "plt.plot( xx, np.exp(xx) - 2 * xx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Sequential( [ \n",
    "        Dense( 1, activation='relu', input_shape=(1,) ), \n",
    "        Dense( 1, activation='relu' )\n",
    "] )\n",
    "\n",
    "def lf( M, x ):\n",
    "    return tf.math.exp( M(x) ) - 2 * M(x)\n",
    "\n",
    "x = tf.reshape( tf.Variable(1.0), (1,1) )\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        L = lambda : lf(M,x)\n",
    "        #fl = lambda : (x-1) ** 2\n",
    "        # loss = ( x - 1 ) ** 2\n",
    "        optimizer.minimize( L, var_list=M.weights )\n",
    "        #grads = tape.gradient( loss, x )\n",
    "        #optimizer.apply_gradients( zip( [grads], [x] ) )\n",
    "        print(M(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = tf.Variable([4.])   \n",
    "\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        fl = lambda : (x-1) ** 2\n",
    "        loss = ( x - 1 ) ** 2\n",
    "        optimizer.minimize( fl, var_list=[x] )\n",
    "        #grads = tape.gradient( loss, x )\n",
    "        #optimizer.apply_gradients( zip( [grads], [x] ) )\n",
    "        print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.optimizers.Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, -1]\n",
      "[1, 1, 0]\n",
      "[2, 2, 1]\n",
      "[3, 3, 2]\n",
      "[4, 4, 3]\n",
      "[5, 5, 4]\n",
      "[6, 6, 5]\n",
      "[7, 7, 6]\n",
      "[8, 8, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aba'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solution().longestPalindrome('abacadaba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
