{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-network Implementation\n",
    "Following example from Week 4 in Practical Reinforcment Learning (Coursera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import envs\n",
    "from envs.Simple2dEnv import Simple2dEnv\n",
    "importlib.reload(envs.Simple2dEnv)\n",
    "from envs.Simple2dEnv import Simple2dEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dim = 2    # number of assets\n",
    "trading_days_per_year = 1\n",
    "risk_free_rate = 0.01\n",
    "utility = lambda x : -np.exp( -2 * x )\n",
    "\n",
    "env = Simple2dEnv(\n",
    "                    dim=dim, \n",
    "                    utility=utility,  \n",
    "                    risk_free_rate=risk_free_rate, \n",
    "                    trading_days_per_year=trading_days_per_year\n",
    "                )\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name, state_dim, action_dim, epsilon=0, reuse=False):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        \n",
    "        state_shape = (state_dim + action_dim,)\n",
    "        \n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.model = Sequential([\n",
    "                Dense( 16, activation='relu', input_shape=state_shape ),\n",
    "                Dense( 16, activation='relu' ),\n",
    "                Dense(  1, activation='sigmoid' )\n",
    "            ])                 \n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.qvalues_t = self.get_symbolic_qvalues( self.state_t )\n",
    "            \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_symbolic_qvalues(self, state_t):\n",
    "        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n",
    "        \n",
    "        #< apply your network layers here >\n",
    "        #qvalues = < symbolic tensor for q-values >\n",
    "        qvalues = self.model(state_t)\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        \n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        \n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = np.random.uniform(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
