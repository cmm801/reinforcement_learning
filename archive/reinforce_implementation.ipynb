{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import envs\n",
    "from envs.Simple2dEnv import Simple2dEnv\n",
    "importlib.reload(envs.Simple2dEnv)\n",
    "from envs.Simple2dEnv import Simple2dEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "action_dim = 2                           # number of assets\n",
    "trading_days_per_year = 1                # Number of trading days in a year\n",
    "risk_free_rate = 0.01                    # Annualized risk-free rate\n",
    "t_max = 100                              # Maximum number of steps in a session\n",
    "learning_rate = 0.01\n",
    "\n",
    "# User-specified utility function\n",
    "ARRA = lambda x, L : 1/L * ( x ** (1/L) - 1 )\n",
    "EXPRA = lambda x, L : -np.exp( -L * x )\n",
    "utility = lambda x : ARRA(x,8)\n",
    "\n",
    "entropy_weight = 0.0\n",
    "\n",
    "gamma = np.exp(-risk_free_rate/trading_days_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Simple2dEnv(\n",
    "                    dim=action_dim, \n",
    "                    utility=utility,  \n",
    "                    risk_free_rate=risk_free_rate, \n",
    "                    trading_days_per_year=trading_days_per_year\n",
    "                )\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "n_hidden_0 = 8\n",
    "n_hidden_1 = 8\n",
    "\n",
    "# <define network graph using raw tf or any deep learning library>\n",
    "model = Sequential( [\n",
    "            Dense(n_hidden_0,  activation='relu', input_shape=state_dim ),\n",
    "            #Dense(n_hidden_1,  activation='relu'),\n",
    "            Dense(action_dim,  activation='relu' ),\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 32\n",
    "\n",
    "for j in range(250):\n",
    "    X_train = tf.stack( [ env.observation_space.sample() for _ in range(N_train) ] )\n",
    "    y_train = tf.random.normal( shape=(N_train,action_dim), mean=1.0, stddev=0.1 )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=1, verbose=0 )\n",
    "\n",
    "    if j % 50 == 0:\n",
    "        print( [ j, np.mean(model(X_train).numpy()) ] )\n",
    "        \n",
    "loss, mae, mse = model.evaluate(X_train, y_train, verbose=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MIN_DIRICHLET_PARAM_ = 0.01\n",
    "_MAX_DIRICHLET_PARAM_ = 100\n",
    "\n",
    "# Get the parameters describing the distribution \\pi(a|s)\n",
    "def get_distribution_parameters( model, _states ):\n",
    "    alpha = model(_states)\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def get_distribution( model, _states ):\n",
    "    \n",
    "    alpha = get_distribution_parameters( model, _states=_states )\n",
    "        \n",
    "    # Clip the parameters to prevent exploding gradients\n",
    "    clipped_alpha = tf.clip_by_value( alpha, _MIN_DIRICHLET_PARAM_, _MAX_DIRICHLET_PARAM_ )    \n",
    "    \n",
    "    dist = tfp.distributions.Dirichlet(clipped_alpha)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "\n",
    "def calc_entropy( model, _states ):\n",
    "    \n",
    "    dist = get_distribution( model, _states=_states )\n",
    "    ent = dist.entropy()\n",
    "    \n",
    "    return ent\n",
    "\n",
    "\n",
    "# Utility function to pick portfolio weights in one given state\n",
    "def choose_weights_from_policy( model, _states ):     \n",
    "    dist = get_distribution( model, _states=_states )\n",
    "    weights = dist.sample(1)\n",
    "    weights = tf.reshape( weights, shape=(tf.size(weights),) )\n",
    "    \n",
    "    log_prob = dist.log_prob( weights )\n",
    "    return weights, log_prob\n",
    "\n",
    "\n",
    "# Utility function to pick action in one given state\n",
    "def choose_action_from_policy( model, _states ):     \n",
    "    weights, log_prob = choose_weights_from_policy( model, _states=_states )\n",
    "    action = weights[:-1]\n",
    "    return action, log_prob\n",
    "\n",
    "\n",
    "# Calculate the objective function for the REINFORCE algorithm\n",
    "def calc_objective_function( model, _rewards, _logprobs, _gamma ):\n",
    "    \n",
    "    cum_rewards = get_cumulative_rewards(_rewards=_rewards, _gamma=_gamma )\n",
    "    J = tf.reduce_mean( _logprobs * cum_rewards )\n",
    "    return J\n",
    "\n",
    "\n",
    "def calc_loss( model, _states, _rewards, _logprobs, _gamma ):\n",
    "    \n",
    "    entropy = calc_entropy( model, _states )\n",
    "    J = calc_objective_function( model, _rewards=_rewards, \n",
    "                                    _logprobs=_logprobs, _gamma=_gamma )\n",
    "    \n",
    "    loss = -J - entropy_weight * entropy\n",
    "    print(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step( model, optimizer, _states, _logprobs, _rewards, _gamma):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "\n",
    "    loss_fun = lambda : calc_loss( model, _states=_states, \n",
    "                        _rewards=_rewards, _logprobs=_logprobs, _gamma=_gamma )\n",
    "    \n",
    "    current_loss = optimizer.minimize( loss_fun, var_list=model.weights )\n",
    "    return current_loss\n",
    "\n",
    "\n",
    "def get_cumulative_rewards(_rewards, _gamma ):\n",
    "\n",
    "    # Calculate the discount factor. Assumes the first rewards are earliest    \n",
    "    cum_rewards = [np.nan] * len(_rewards)\n",
    "    cum_rewards[-1] = _rewards[-1]\n",
    "    for t in range(1, len(_rewards)):\n",
    "        cum_rewards[-(t+1)] = _rewards[-(t+1)] + _gamma * cum_rewards[-t] \n",
    "\n",
    "    return np.array( cum_rewards )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session( model, t_max ):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = obs[:,np.newaxis].T\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka \\pi(a|s)\n",
    "        action, log_prob = choose_action_from_policy( model, obs )\n",
    "\n",
    "        # Move to next state\n",
    "        new_obs, r, done, info = env.step(action )\n",
    "        new_obs = new_obs[:,np.newaxis].T\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    state_mtx = tf.stack( states )\n",
    "    action_mtx = tf.stack( actions )\n",
    "    logprob_mtx = tf.stack( log_probs )\n",
    "    reward_mtx = tf.stack( rewards )\n",
    "    \n",
    "    train_step( model, optimizer=optimizer, _states=state_mtx, \n",
    "                   _logprobs=logprob_mtx, _rewards=reward_mtx, _gamma=gamma )\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "mean_rewards = []\n",
    "fig = plt.figure()\n",
    "for i in range(100): \n",
    "\n",
    "    rewards = [ generate_session(model, t_max) for _ in range(20)]  # generate new sessions\n",
    "    mean_rewards.append( np.mean(rewards) )\n",
    "    clear_output()\n",
    "    plt.plot( mean_rewards )\n",
    "    plt.pause(0.05)\n",
    "    \n",
    "    print( \"{}: mean reward: {}\".format(i, np.mean(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays to record session\n",
    "states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "obs = env.reset()\n",
    "obs = obs[:,np.newaxis].T\n",
    "\n",
    "for t in range(t_max):\n",
    "\n",
    "    # action probabilities array aka \\pi(a|s)\n",
    "    action, log_prob = choose_action_from_policy( model, obs )\n",
    "\n",
    "    # Move to next state\n",
    "    new_obs, r, done, info = env.step(action )\n",
    "    new_obs = new_obs[:,np.newaxis].T\n",
    "\n",
    "    # record session history to train later\n",
    "    states.append(obs)\n",
    "    actions.append(action)\n",
    "    log_probs.append(log_prob)\n",
    "    rewards.append(r)\n",
    "\n",
    "    obs = new_obs\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "state_mtx = tf.stack( states )\n",
    "action_mtx = tf.stack( actions )\n",
    "logprob_mtx = tf.stack( log_probs )\n",
    "reward_mtx = tf.stack( rewards )\n",
    "\n",
    "loss = train_step( model, optimizer=optimizer, _states=state_mtx, \n",
    "                   _logprobs=logprob_mtx, _rewards=reward_mtx, _gamma=gamma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = calc_entropy( model, state_mtx )\n",
    "print(np.mean(entropy) / np.mean(reward_mtx) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_states = state_mtx\n",
    "_rewards = reward_mtx \n",
    "_logprobs = logprob_mtx\n",
    "_gamma = gamma\n",
    "entropy = calc_entropy( model, _states )\n",
    "J = calc_objective_function( model, _rewards=_rewards, \n",
    "                                _logprobs=_logprobs, _gamma=_gamma )\n",
    "\n",
    "-J - entropy_weight * entropy\n",
    "[ np.mean(-J), np.mean(- entropy_weight * entropy) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( action_mtx.numpy(), -J )\n",
    "plt.ylim( -0.002, 0.002 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = calc_entropy( model, state_mtx )\n",
    "J = calc_objective_function( model, action_mtx, reward_mtx, gamma )\n",
    "loss = -J - entropy_weight * entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp.distributions.Dirichlet( [ .01, 100 ] ).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[:,np.newaxis].T\n",
    "\n",
    "mu = env.state_params['mu']\n",
    "sigma = env.state_params['sigma']\n",
    "vols = np.sqrt(np.diag(sigma))\n",
    "print(mu)\n",
    "print(vols)\n",
    "print(mu/vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs, r, done, info = env.step([1.])\n",
    "print( [ new_obs[0], r ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( action_mtx.numpy(), reward_mtx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp.distributions.Normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_rwds = get_cumulative_rewards(_rewards=rewards, _gamma=gamma )\n",
    "plt.hist( logprob_mtx.numpy().flatten() )\n",
    "#plt.hist( cum_rwds.flatten() * logprob_mtx.numpy().flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_rwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
